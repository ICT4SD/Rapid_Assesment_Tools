{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##this is to test if results of model are good ennough\n",
    "\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "dirc = r'C:\\\\Users\\\\DD\\\\OneDrive\\\\Documents\\\\Rapid_Assessment_Tools\\\\Plans_test\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load file to list 'paragraphs' split by paragraph\n",
    "\n",
    "f = open(dirc + 'Bhutan_1_utf8.txt', 'r').read().split(\"\\n\\n\")\n",
    "f = [i.replace('\\n', '') for i in f]\n",
    "\n",
    "paragraphs = []\n",
    "for lines in f:\n",
    "    if len(lines) > 120:\n",
    "        paragraphs.append(lines)\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "p_tokenized = [[word.lower() for word in word_tokenize(document.decode('utf-8'))] for document in paragraphs]\n",
    "len(p_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'targeted', u'poverty', u'reduction', u'interventions', u',', u'four', u'villages', u'namely', u'dangochang', u',', u'chebesa', u',', u'barchong', u'tshochekha', u'supported', u'reap', u'phase', u'ii', u'.', u'income', u'generating', u'sustainable', u'livelihood', u'activities', u',', u'capacity', u'development', u'establishment', u'self-help', u'groups/cooperatives', u'interventions', u'village', u'level', u'additional', u'support', u'household', u'levels', u'provided', u'poorest', u'households', u'within', u'village', u'.', u'funds', u'programme', u'directly', u'provided', u'centre', u'.']\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "p_no_stopwords = [[word for word in document if not word in english_stopwords] for document in p_tokenized]\n",
    "print p_no_stopwords[1220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'targeted', u'poverty', u'reduction', u'interventions', u'four', u'villages', u'namely', u'dangochang', u'chebesa', u'barchong', u'tshochekha', u'supported', u'reap', u'phase', u'ii', u'income', u'generating', u'sustainable', u'livelihood', u'activities', u'capacity', u'development', u'establishment', u'self-help', u'groups/cooperatives', u'interventions', u'village', u'level', u'additional', u'support', u'household', u'levels', u'provided', u'poorest', u'households', u'within', u'village', u'funds', u'programme', u'directly', u'provided', u'centre']\n"
     ]
    }
   ],
   "source": [
    "#remove punctuations\n",
    "english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', '...'] #add '...'\n",
    "p_filtered = [[word for word in document if not word in english_punctuations] for document in p_no_stopwords]\n",
    "print p_filtered[1220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'target', u'poverty', u'reduc', u'interv', u'four', u'vil', u'nam', u'dangochang', u'chebes', u'barchong', u'tshochekha', u'support', u'reap', u'phas', u'ii', u'incom', u'gen', u'sustain', u'liv', u'act', u'capac', u'develop', u'est', u'self-help', u'groups/cooperative', u'interv', u'vil', u'level', u'addit', u'support', u'household', u'level', u'provid', u'poorest', u'household', u'within', u'vil', u'fund', u'program', u'direct', u'provid', u'cent']\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "p_stemmed = [[st.stem(word) for word in docment] for docment in p_filtered]\n",
    "print p_stemmed[1220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove word with low frequency\n",
    "all_stems = sum(p_stemmed, [])\n",
    "stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)\n",
    "texts = [[stem for stem in text if stem not in stems_once] for text in p_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1 By 2030, eradicate extreme poverty for all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2 By 2030, reduce at least by half the propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3 Implement nationally appropriate social pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4 By 2030, ensure that all men and women, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5 By 2030, build the resilience of the poor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1 By 2030, eradicate extreme poverty for all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2 By 2030, reduce at least by half the propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3 Implement nationally appropriate social pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4 By 2030, ensure that all men and women, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5 By 2030, build the resilience of the poor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## load goals\n",
    "\n",
    "#use SDG as query\n",
    "\n",
    "targets = pd.read_excel(dirc +'SDG sub goals.xlsx', names = ['ID', 'Target'],sheetname = 'Sheet1')\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.4 By 2030, ensure sustainable food production systems and implement resilient agricultural practices that increase productivity and production, that help maintain ecosystems, that strengthen capacity for adaptation to climate change, extreme weather, drought, flooding and other disasters and that progressively improve land and soil quality\\xa0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list with goal contents\n",
    "\n",
    "goal = []\n",
    "for doc in targets.iloc[:]['Target']:\n",
    "    goal.append(doc)\n",
    "\n",
    "goal_id = targets.iloc[:]['ID']\n",
    "goal[10] #test goal 2.4\n",
    "#goal_id[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "goal_tokenized = [[word.lower() for word in word_tokenize(document)] for document in goal]\n",
    "\n",
    "#remove stopwords\n",
    "goal_no_sw = [[word for word in document if not word in english_stopwords] for document in goal_tokenized]\n",
    "\n",
    "#stemming\n",
    "goal_stemmed = [[st.stem(word) for word in l] for l in goal_no_sw]\n",
    "len(goal_stemmed[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_query = goal_stemmed #_frequent\n",
    "len(g_query[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_goal(texts, query, num=100):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num)\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    ml_bhutan = query\n",
    "    ml_bow = dictionary.doc2bow(ml_bhutan)\n",
    "    ml_lsi = lsi[ml_bow]\n",
    "    sims = index[ml_lsi]\n",
    "    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    print sort_sims[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,651 : INFO : built Dictionary(3046 unique tokens: [u'lhuents', u'1200', u'bear', u'four', u'phajod']...) from 1331 documents (total 56199 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,756 : INFO : collecting document frequencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,759 : INFO : PROGRESS: processing document #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,796 : INFO : calculating IDF weights for 1331 documents and 3045 features (44303 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,805 : INFO : using serial LSI version on this node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,808 : INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,934 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,970 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:05,973 : INFO : 1st phase: constructing (3046L, 200L) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,025 : INFO : orthonormalizing (3046L, 200L) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,240 : INFO : 2nd phase: running dense svd on (200L, 1331L) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,293 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,296 : INFO : keeping 100 factors (discarding 25.303% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,312 : INFO : processed documents up to #1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,319 : INFO : topic #0(6.418): 0.166*\"develop\" + 0.145*\"dzongkh\" + 0.139*\"perc\" + 0.135*\"poverty\" + 0.134*\"program\" + 0.131*\"plan\" + 0.128*\"produc\" + 0.115*\"serv\" + 0.114*\"ii\" + 0.114*\"tour\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,322 : INFO : topic #1(4.390): -0.261*\"ii\" + -0.229*\"vil\" + -0.219*\"interv\" + -0.193*\"poverty\" + -0.185*\"development°±\" + -0.184*\"°∞self-reliance\" + -0.177*\"program\" + -0.171*\"keep\" + 0.171*\"proport\" + -0.147*\"green\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,325 : INFO : topic #2(4.171): 0.284*\"proport\" + 0.258*\"vil\" + 0.228*\"household\" + 0.189*\"1\" + 0.169*\"pop\" + 0.163*\"2\" + 0.150*\"3\" + 0.145*\"support\" + -0.114*\"sect\" + 0.112*\"interv\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,328 : INFO : topic #3(4.074): 0.243*\"perc\" + -0.193*\"farm\" + 0.192*\"vil\" + -0.182*\"market\" + -0.179*\"facil\" + -0.172*\"produc\" + -0.162*\"agricult\" + -0.151*\"livestock\" + 0.150*\"level\" + -0.142*\"tour\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,332 : INFO : topic #4(3.873): -0.291*\"proport\" + -0.243*\"pop\" + 0.175*\"vil\" + -0.172*\"ii\" + -0.168*\"development°±\" + -0.167*\"°∞self-reliance\" + -0.160*\"tour\" + 0.154*\"support\" + -0.154*\"keep\" + 0.139*\"market\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,339 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-10 00:26:06,486 : INFO : creating matrix with 1331 documents and 100 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1200, 0.64878607), (1073, 0.64179593), (955, 0.59496409), (888, 0.58894688), (1247, 0.58887935), (913, 0.58774322), (961, 0.57525998), (546, 0.57305968), (1100, 0.5719372), (1205, 0.56955922), (1258, 0.54628813), (1223, 0.52869928), (1263, 0.5251528), (893, 0.51373851), (851, 0.51371974), (531, 0.50209439), (985, 0.49586684), (592, 0.49570248), (873, 0.49462157), (146, 0.492322), (547, 0.48060277), (981, 0.4805066), (963, 0.4751941), (1044, 0.47515059), (548, 0.4746733), (536, 0.47387207), (794, 0.47316504), (1265, 0.46534264), (1242, 0.4641923), (1081, 0.45105031), (1029, 0.44932601), (568, 0.44407195), (847, 0.43337983), (865, 0.43256277), (939, 0.42699033), (1215, 0.42292976), (1142, 0.42165905), (957, 0.41832817), (796, 0.41700244), (544, 0.41681421), (1054, 0.4165116), (277, 0.41592604), (930, 0.41333434), (321, 0.41274351), (448, 0.40837723), (209, 0.40825102), (1079, 0.40667164), (609, 0.40137777), (60, 0.40118441), (1147, 0.39679193), (362, 0.39623559), (1278, 0.3955735), (1105, 0.39548606), (1090, 0.39476499), (21, 0.39387476), (1183, 0.39172697), (366, 0.39037362), (538, 0.38775501), (760, 0.38466507), (1030, 0.38370222), (542, 0.38347453), (143, 0.38103914), (1121, 0.37935638), (954, 0.37554979), (363, 0.37395155), (790, 0.37373725), (842, 0.36497098), (268, 0.36454681), (539, 0.36178449), (458, 0.35987145), (62, 0.35424453), (571, 0.34828889), (905, 0.34798551), (1180, 0.34134996), (198, 0.33875555), (1004, 0.33411264), (294, 0.33293995), (983, 0.33201057), (910, 0.33183864), (136, 0.331637), (886, 0.33034259), (1217, 0.32911122), (836, 0.32754734), (596, 0.32634115), (628, 0.32610965), (208, 0.32576245), (1053, 0.32507053), (205, 0.32448828), (369, 0.32444817), (537, 0.32315639), (885, 0.32211483), (1254, 0.32169184), (147, 0.32162854), (368, 0.31960082), (206, 0.31813091), (581, 0.3165051), (494, 0.31610435), (797, 0.31541434), (1314, 0.31141037), (767, 0.30761313)]\n"
     ]
    }
   ],
   "source": [
    "match_goal(texts, g_query[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trongsa has suitable climatic conditions which is conducive for enhancing agricultural and horticultural products, besides livestock products such as poultry, piggery and dairy backyard farming. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[1200] #ranking 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With a strong emphasis on improving the health and education outcomes, achieving progress in the HAI may not be a major constraint other than improving the adult literacy rate which may pose a challenge. However, improving the EVI will require special efforts particularly in addressing economic imbalances, productive capacities and diversification challenges. This will not be easy. Hence, the goodwill, support and cooperation of our development partner in the coming years will be critical for Bhutan. Furthermore, even beyond 2020, Bhutan may require continued cooperation and support from its development partners to ensure that the transition will help establish a solid economic foundation that is dynamic and resilient. '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[147] #ranking 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.4 By 2030, ensure sustainable food production systems and implement resilient agricultural practices that increase productivity and production, that help maintain ecosystems, that strengthen capacity for adaptation to climate change, extreme weather, drought, flooding and other disasters and that progressively improve land and soil quality\\xa0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing goal 2.4\n",
    "goal[10]\n",
    "#goal_id[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
